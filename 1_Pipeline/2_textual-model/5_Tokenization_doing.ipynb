{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2028a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81aea1f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30f897320b9e42f39c5d7d5a9f941c9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/534172 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = 'cardiffnlp/twitter-roberta-base-hate-latest'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# Load and prepare data\n",
    "df = pd.read_csv(\"data/merged_wo_emojis.csv\")\n",
    "df['labels'] = df['labels'].astype(int)\n",
    "\n",
    "dataset = Dataset.from_pandas(df)\n",
    "tokenized_dataset = dataset.map(\n",
    "    lambda x: tokenizer(x['clean_text'], padding=\"max_length\", truncation=True, max_length=70), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7548c09e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'labels', 'word_count', 'clean_text', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 534172\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc7afaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unused columns\n",
    "# cols to remove after my bertweet: ['text', 'clean_text', 'word_count', 'token_type_ids']\n",
    "# cols to remove after cardiffnlp: ['text', 'clean_text', 'word_count']\n",
    "\n",
    "tokenized_dataset = tokenized_dataset.remove_columns(['text', 'clean_text', 'word_count'])\n",
    "\n",
    "# Set format for PyTorch\n",
    "tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1022a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to pandas for stratified splitting\n",
    "df = tokenized_dataset.to_pandas()\n",
    "\n",
    "# Split while preserving label distribution\n",
    "train_df, test_df = train_test_split(\n",
    "    df,\n",
    "    test_size=10_000,\n",
    "    stratify=df[\"labels\"],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Convert back to HF DatasetDict\n",
    "train_dataset = Dataset.from_pandas(train_df).remove_columns([\"__index_level_0__\"])\n",
    "test_dataset = Dataset.from_pandas(test_df).remove_columns([\"__index_level_0__\"])\n",
    "\n",
    "tokenized_dataset = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"test\": test_dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb04d1e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['labels', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 524172\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['labels', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ef537d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Convert train_dataset to pandas\n",
    "train_df = tokenized_dataset[\"train\"].to_pandas()\n",
    "\n",
    "# Stratified 90% train / 10% val\n",
    "train_split, val_split = train_test_split(\n",
    "    train_df, \n",
    "    test_size=30_000, \n",
    "    stratify=train_df[\"labels\"], \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "from datasets import DatasetDict, Dataset\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_split).remove_columns([\"__index_level_0__\"])\n",
    "val_dataset = Dataset.from_pandas(val_split).remove_columns([\"__index_level_0__\"])\n",
    "\n",
    "\n",
    "tokenized_dataset[\"train\"] = train_dataset\n",
    "tokenized_dataset[\"validation\"] = val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ef98aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Label Distribution: Counter({0: 331917, 1: 162255})\n",
      "Val Label Distribution: Counter({0: 20150, 1: 9850})\n",
      "Test Label Distribution: Counter({0: 6717, 1: 3283})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "print(\"Train Label Distribution:\", Counter(tokenized_dataset['train']['labels']))\n",
    "print(\"Val Label Distribution:\", Counter(tokenized_dataset['validation']['labels']))\n",
    "print(\"Test Label Distribution:\", Counter(tokenized_dataset['test']['labels']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d41511e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0efa1820",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['labels', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 494172\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['labels', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['labels', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 30000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73392085",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecab08c95ae84ab8b3c53bba7a1d2019",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/494172 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcccc21888c24c52a70efc83259feeca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b54cf7c122924c33ad613739335a5953",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/30000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset.save_to_disk(\"data/tokenized_merged_wo_emojis_cardiffnlp_latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf59a194",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'f:\\\\0_System Folders\\\\Desktop\\\\SDP\\\\3_Coding\\\\1_Textual\\\\Working\\\\data\\\\tokenized_merged_wo_emojis_cardiffnlp_latest.zip'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "shutil.make_archive(\"data/tokenized_merged_wo_emojis_cardiffnlp_latest\", 'zip', \"data/tokenized_merged_wo_emojis_cardiffnlp_latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31511b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_from_disk\n",
    "\n",
    "# tokenized_dataset = load_from_disk(\"data/tokenized_superset_IO\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d397b270",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import compare_tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0878ca80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer\n",
    "\n",
    "# # Define model names and paths\n",
    "# model_names = {\n",
    "#     \"BERTweet\": \"vinai/bertweet-base\",\n",
    "#     \"RoBERTa\": \"roberta-base\",\n",
    "#     \"BERT\": \"bert-base-uncased\"\n",
    "# }\n",
    "\n",
    "# def detect_subword_prefix(tokenizer):\n",
    "#     # Check for common subword prefixes like '##' used in WordPiece\n",
    "#     tokens = tokenizer.tokenize(\"unbelievable\")\n",
    "#     for token in tokens:\n",
    "#         if token.startswith(\"##\"):\n",
    "#             return \"## (WordPiece)\"\n",
    "#     return \"None or different (e.g. BPE)\"\n",
    "\n",
    "# # Build comparison table\n",
    "# comparison = []\n",
    "\n",
    "# for name, model in model_names.items():\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(model, use_fast=True)\n",
    "\n",
    "#     entry = {\n",
    "#         \"Model\": name,\n",
    "#         \"Base Vocab Size\": tokenizer.vocab_size,\n",
    "#         \"Total Tokens (with specials)\": len(tokenizer),\n",
    "#         \"Tokenizer Type\": type(tokenizer).__name__,\n",
    "#         \"Special Tokens\": list(tokenizer.special_tokens_map.keys()),\n",
    "#         \"Max Length\": tokenizer.model_max_length,\n",
    "#         \"Fast Tokenizer\": \"Yes\" if tokenizer.is_fast else \"No\",\n",
    "#         \"Subword Prefix\": detect_subword_prefix(tokenizer)\n",
    "#     }\n",
    "\n",
    "#     comparison.append(entry)\n",
    "\n",
    "# # Display as markdown\n",
    "# print(\"### ğŸ” Tokenizer Comparison Table\\n\")\n",
    "# print(f\"{'Model':<10} | {'Vocab':<6} | {'Total':<6} | {'Type':<20} | {'Lowercasing':<12} | {'Fast':<4} | {'MaxLen':<6} | {'Subword':<20} | Special Tokens\")\n",
    "# print(\"-\" * 130)\n",
    "# for row in comparison:\n",
    "#     print(f\"{row['Model']:<10} | {row['Base Vocab Size']:<6} | {row['Total Tokens (with specials)']:<6} | {row['Tokenizer Type']:<20} | {row['Lowercasing']:<12} | {row['Fast Tokenizer']:<4} | {row['Max Length']:<6} | {row['Subword Prefix']:<20} | {row['Special Tokens']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7324e53a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Tokenizer Components: bert-base-uncased</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ \u001b[0m\u001b[1;36mTokenizer Components: bert-base-uncased\u001b[0m\u001b[92m â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Component     </span>â”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Description                                                                                     </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> Normalizer    </span>â”‚ BertNormalizer(clean_text=True, handle_chinese_chars=True, strip_accents=None, lowercase=True)  â”‚\n",
       "â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> PreTokenizer  </span>â”‚ BertPreTokenizer()                                                                              â”‚\n",
       "â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> Model         </span>â”‚ WordPiece(unk_token=\"[UNK]\", continuing_subword_prefix=\"##\", max_input_chars_per_word=100,      â”‚\n",
       "â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>â”‚ vocab={\"[PAD]\":0, \"\":1, \"\":2, \"\":3, \"\":4, ...})                                                 â”‚\n",
       "â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> PostProcessor </span>â”‚ TemplateProcessing(single=[SpecialToken(id=\"[CLS]\", type_id=0), Sequence(id=A, type_id=0),      â”‚\n",
       "â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>â”‚ SpecialToken(id=\"[SEP]\", type_id=0)], pair=[SpecialToken(id=\"[CLS]\", type_id=0), Sequence(id=A, â”‚\n",
       "â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>â”‚ type_id=0), SpecialToken(id=\"[SEP]\", type_id=0), Sequence(id=B, type_id=1),                     â”‚\n",
       "â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>â”‚ SpecialToken(id=\"[SEP]\", type_id=1)], special_tokens={\"[CLS]\":SpecialToken(id=\"[CLS]\",          â”‚\n",
       "â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>â”‚ ids=[101], tokens=[\"[CLS]\"]), \"[SEP]\":SpecialToken(id=\"[SEP]\", ids=[102], tokens=[\"[SEP]\"])})   â”‚\n",
       "â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> Decoder       </span>â”‚ WordPiece(prefix=\"##\", cleanup=True)                                                            â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1;35m \u001b[0m\u001b[1;35mComponent    \u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\u001b[1;35m \u001b[0m\u001b[1;35mDescription                                                                                    \u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚\u001b[2m \u001b[0m\u001b[2mNormalizer   \u001b[0m\u001b[2m \u001b[0mâ”‚ BertNormalizer(clean_text=True, handle_chinese_chars=True, strip_accents=None, lowercase=True)  â”‚\n",
       "â”‚\u001b[2m \u001b[0m\u001b[2mPreTokenizer \u001b[0m\u001b[2m \u001b[0mâ”‚ BertPreTokenizer()                                                                              â”‚\n",
       "â”‚\u001b[2m \u001b[0m\u001b[2mModel        \u001b[0m\u001b[2m \u001b[0mâ”‚ WordPiece(unk_token=\"[UNK]\", continuing_subword_prefix=\"##\", max_input_chars_per_word=100,      â”‚\n",
       "â”‚\u001b[2m               \u001b[0mâ”‚ vocab={\"[PAD]\":0, \"\":1, \"\":2, \"\":3, \"\":4, ...})                                                 â”‚\n",
       "â”‚\u001b[2m \u001b[0m\u001b[2mPostProcessor\u001b[0m\u001b[2m \u001b[0mâ”‚ TemplateProcessing(single=[SpecialToken(id=\"[CLS]\", type_id=0), Sequence(id=A, type_id=0),      â”‚\n",
       "â”‚\u001b[2m               \u001b[0mâ”‚ SpecialToken(id=\"[SEP]\", type_id=0)], pair=[SpecialToken(id=\"[CLS]\", type_id=0), Sequence(id=A, â”‚\n",
       "â”‚\u001b[2m               \u001b[0mâ”‚ type_id=0), SpecialToken(id=\"[SEP]\", type_id=0), Sequence(id=B, type_id=1),                     â”‚\n",
       "â”‚\u001b[2m               \u001b[0mâ”‚ SpecialToken(id=\"[SEP]\", type_id=1)], special_tokens={\"[CLS]\":SpecialToken(id=\"[CLS]\",          â”‚\n",
       "â”‚\u001b[2m               \u001b[0mâ”‚ ids=[101], tokens=[\"[CLS]\"]), \"[SEP]\":SpecialToken(id=\"[SEP]\", ids=[102], tokens=[\"[SEP]\"])})   â”‚\n",
       "â”‚\u001b[2m \u001b[0m\u001b[2mDecoder      \u001b[0m\u001b[2m \u001b[0mâ”‚ WordPiece(prefix=\"##\", cleanup=True)                                                            â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Tokenizer Components: roberta-base</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ \u001b[0m\u001b[1;36mTokenizer Components: roberta-base\u001b[0m\u001b[92m â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Component     </span>â”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Description                                                                                     </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> Normalizer    </span>â”‚ None                                                                                            â”‚\n",
       "â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> PreTokenizer  </span>â”‚ ByteLevel(add_prefix_space=False, trim_offsets=True, use_regex=True)                            â”‚\n",
       "â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> Model         </span>â”‚ BPE(dropout=None, unk_token=None, continuing_subword_prefix=\"\", end_of_word_suffix=\"\",          â”‚\n",
       "â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>â”‚ fuse_unk=False, byte_fallback=False, ignore_merges=False, vocab={\"&lt;s&gt;\":0, \"&lt;pad&gt;\":1, \"&lt;/s&gt;\":2,  â”‚\n",
       "â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>â”‚ \"&lt;unk&gt;\":3, \".\":4, ...}, merges=[(\"Ä \", \"t\"), (\"Ä \", \"a\"), (\"h\", \"e\"), (\"i\", \"n\"), (\"r\", \"e\"),     â”‚\n",
       "â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>â”‚ ...])                                                                                           â”‚\n",
       "â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> PostProcessor </span>â”‚ RobertaProcessing(sep=(\"&lt;/s&gt;\", 2), cls=(\"&lt;s&gt;\", 0), trim_offsets=True, add_prefix_space=False)   â”‚\n",
       "â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> Decoder       </span>â”‚ ByteLevel(add_prefix_space=True, trim_offsets=True, use_regex=True)                             â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1;35m \u001b[0m\u001b[1;35mComponent    \u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\u001b[1;35m \u001b[0m\u001b[1;35mDescription                                                                                    \u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚\u001b[2m \u001b[0m\u001b[2mNormalizer   \u001b[0m\u001b[2m \u001b[0mâ”‚ None                                                                                            â”‚\n",
       "â”‚\u001b[2m \u001b[0m\u001b[2mPreTokenizer \u001b[0m\u001b[2m \u001b[0mâ”‚ ByteLevel(add_prefix_space=False, trim_offsets=True, use_regex=True)                            â”‚\n",
       "â”‚\u001b[2m \u001b[0m\u001b[2mModel        \u001b[0m\u001b[2m \u001b[0mâ”‚ BPE(dropout=None, unk_token=None, continuing_subword_prefix=\"\", end_of_word_suffix=\"\",          â”‚\n",
       "â”‚\u001b[2m               \u001b[0mâ”‚ fuse_unk=False, byte_fallback=False, ignore_merges=False, vocab={\"<s>\":0, \"<pad>\":1, \"</s>\":2,  â”‚\n",
       "â”‚\u001b[2m               \u001b[0mâ”‚ \"<unk>\":3, \".\":4, ...}, merges=[(\"Ä \", \"t\"), (\"Ä \", \"a\"), (\"h\", \"e\"), (\"i\", \"n\"), (\"r\", \"e\"),     â”‚\n",
       "â”‚\u001b[2m               \u001b[0mâ”‚ ...])                                                                                           â”‚\n",
       "â”‚\u001b[2m \u001b[0m\u001b[2mPostProcessor\u001b[0m\u001b[2m \u001b[0mâ”‚ RobertaProcessing(sep=(\"</s>\", 2), cls=(\"<s>\", 0), trim_offsets=True, add_prefix_space=False)   â”‚\n",
       "â”‚\u001b[2m \u001b[0m\u001b[2mDecoder      \u001b[0m\u001b[2m \u001b[0mâ”‚ ByteLevel(add_prefix_space=True, trim_offsets=True, use_regex=True)                             â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Tokenizer Components: distilbert-base-cased</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ \u001b[0m\u001b[1;36mTokenizer Components: distilbert-base-cased\u001b[0m\u001b[92m â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Component     </span>â”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Description                                                                                     </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> Normalizer    </span>â”‚ BertNormalizer(clean_text=True, handle_chinese_chars=True, strip_accents=None, lowercase=False) â”‚\n",
       "â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> PreTokenizer  </span>â”‚ BertPreTokenizer()                                                                              â”‚\n",
       "â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> Model         </span>â”‚ WordPiece(unk_token=\"[UNK]\", continuing_subword_prefix=\"##\", max_input_chars_per_word=100,      â”‚\n",
       "â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>â”‚ vocab={\"[PAD]\":0, \"\":1, \"\":2, \"\":3, \"\":4, ...})                                                 â”‚\n",
       "â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> PostProcessor </span>â”‚ TemplateProcessing(single=[SpecialToken(id=\"[CLS]\", type_id=0), Sequence(id=A, type_id=0),      â”‚\n",
       "â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>â”‚ SpecialToken(id=\"[SEP]\", type_id=0)], pair=[SpecialToken(id=\"[CLS]\", type_id=0), Sequence(id=A, â”‚\n",
       "â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>â”‚ type_id=0), SpecialToken(id=\"[SEP]\", type_id=0), Sequence(id=B, type_id=1),                     â”‚\n",
       "â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>â”‚ SpecialToken(id=\"[SEP]\", type_id=1)], special_tokens={\"[CLS]\":SpecialToken(id=\"[CLS]\",          â”‚\n",
       "â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>â”‚ ids=[101], tokens=[\"[CLS]\"]), \"[SEP]\":SpecialToken(id=\"[SEP]\", ids=[102], tokens=[\"[SEP]\"])})   â”‚\n",
       "â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> Decoder       </span>â”‚ WordPiece(prefix=\"##\", cleanup=True)                                                            â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1;35m \u001b[0m\u001b[1;35mComponent    \u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\u001b[1;35m \u001b[0m\u001b[1;35mDescription                                                                                    \u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚\u001b[2m \u001b[0m\u001b[2mNormalizer   \u001b[0m\u001b[2m \u001b[0mâ”‚ BertNormalizer(clean_text=True, handle_chinese_chars=True, strip_accents=None, lowercase=False) â”‚\n",
       "â”‚\u001b[2m \u001b[0m\u001b[2mPreTokenizer \u001b[0m\u001b[2m \u001b[0mâ”‚ BertPreTokenizer()                                                                              â”‚\n",
       "â”‚\u001b[2m \u001b[0m\u001b[2mModel        \u001b[0m\u001b[2m \u001b[0mâ”‚ WordPiece(unk_token=\"[UNK]\", continuing_subword_prefix=\"##\", max_input_chars_per_word=100,      â”‚\n",
       "â”‚\u001b[2m               \u001b[0mâ”‚ vocab={\"[PAD]\":0, \"\":1, \"\":2, \"\":3, \"\":4, ...})                                                 â”‚\n",
       "â”‚\u001b[2m \u001b[0m\u001b[2mPostProcessor\u001b[0m\u001b[2m \u001b[0mâ”‚ TemplateProcessing(single=[SpecialToken(id=\"[CLS]\", type_id=0), Sequence(id=A, type_id=0),      â”‚\n",
       "â”‚\u001b[2m               \u001b[0mâ”‚ SpecialToken(id=\"[SEP]\", type_id=0)], pair=[SpecialToken(id=\"[CLS]\", type_id=0), Sequence(id=A, â”‚\n",
       "â”‚\u001b[2m               \u001b[0mâ”‚ type_id=0), SpecialToken(id=\"[SEP]\", type_id=0), Sequence(id=B, type_id=1),                     â”‚\n",
       "â”‚\u001b[2m               \u001b[0mâ”‚ SpecialToken(id=\"[SEP]\", type_id=1)], special_tokens={\"[CLS]\":SpecialToken(id=\"[CLS]\",          â”‚\n",
       "â”‚\u001b[2m               \u001b[0mâ”‚ ids=[101], tokens=[\"[CLS]\"]), \"[SEP]\":SpecialToken(id=\"[SEP]\", ids=[102], tokens=[\"[SEP]\"])})   â”‚\n",
       "â”‚\u001b[2m \u001b[0m\u001b[2mDecoder      \u001b[0m\u001b[2m \u001b[0mâ”‚ WordPiece(prefix=\"##\", cleanup=True)                                                            â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Tokenizer Components: vinai/bertweet-base</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ \u001b[0m\u001b[1;36mTokenizer Components: vinai/bertweet-base\u001b[0m\u001b[92m â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">âš ï¸ This tokenizer does not use a fast backend. No component info available.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31mâš ï¸ This tokenizer does not use a fast backend. No component info available.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "\n",
    "console = Console()\n",
    "\n",
    "# Tokenizer models to inspect\n",
    "tokenizer_names = [\n",
    "    \"bert-base-uncased\",\n",
    "    \"roberta-base\",\n",
    "    \"distilbert-base-cased\",\n",
    "    'vinai/bertweet-base'\n",
    "]\n",
    "\n",
    "for name in tokenizer_names:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(name)\n",
    "    console.rule(f\"[bold cyan]Tokenizer Components: {name}\")\n",
    "\n",
    "    if hasattr(tokenizer, 'backend_tokenizer'):\n",
    "        backend = tokenizer.backend_tokenizer\n",
    "\n",
    "        table = Table(show_header=True, header_style=\"bold magenta\")\n",
    "        table.add_column(\"Component\", style=\"dim\")\n",
    "        table.add_column(\"Description\", overflow=\"fold\")\n",
    "\n",
    "        table.add_row(\"Normalizer\", str(backend.normalizer))\n",
    "        table.add_row(\"PreTokenizer\", str(backend.pre_tokenizer))\n",
    "        table.add_row(\"Model\", str(backend.model))\n",
    "        table.add_row(\"PostProcessor\", str(backend.post_processor))\n",
    "        table.add_row(\"Decoder\", str(backend.decoder))\n",
    "\n",
    "        console.print(table)\n",
    "    else:\n",
    "        console.print(\"[red]âš ï¸ This tokenizer does not use a fast backend. No component info available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5c93395",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "ğŸ“ Testing Sentence:\n",
       "OMG ğŸ˜‚ I just met ElonMusk at Starbucks!!! #AI #Future ğŸš€\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "ğŸ“ Testing Sentence:\n",
       "OMG ğŸ˜‚ I just met ElonMusk at Starbucks!!! #AI #Future ğŸš€\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                       ğŸ§¬ Token IDs &amp; Tokens for OMG example                                       </span>\n",
       "â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">       </span>â”ƒ<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">            </span>â”ƒ<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">            </span>â”ƒ<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> Distilbert </span>â”ƒ<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> Distilbert </span>â”ƒ<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">  RoBERTa   </span>â”ƒ<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">             </span>â”ƒ<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">  BT vinai  </span>â”ƒ<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">             </span>â”ƒ\n",
       "â”ƒ<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">       </span>â”ƒ<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> Bert Cased </span>â”ƒ<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> Bert Cased </span>â”ƒ<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">   Cased    </span>â”ƒ<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> Cased      </span>â”ƒ<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> Base Token </span>â”ƒ<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> RoBERTa     </span>â”ƒ<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">   vbase    </span>â”ƒ<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> BT vinai    </span>â”ƒ\n",
       "â”ƒ<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> Index </span>â”ƒ<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">  Token ID  </span>â”ƒ<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> Token      </span>â”ƒ<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">  Token ID  </span>â”ƒ<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> Token      </span>â”ƒ<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">     ID     </span>â”ƒ<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> Base Token  </span>â”ƒ<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">  Token ID  </span>â”ƒ<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> vbase Token </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚   0   â”‚    101     â”‚ [CLS]      â”‚    101     â”‚ [CLS]      â”‚     0      â”‚ &lt;s&gt;         â”‚     0      â”‚ &lt;s&gt;         â”‚\n",
       "â”‚   1   â”‚    152     â”‚ O          â”‚    152     â”‚ O          â”‚    3765    â”‚ OM          â”‚    1115    â”‚ OMG         â”‚\n",
       "â”‚   2   â”‚   14666    â”‚ ##MG       â”‚   14666    â”‚ ##MG       â”‚    534     â”‚ G           â”‚     3      â”‚ &lt;unk&gt;       â”‚\n",
       "â”‚   3   â”‚    100     â”‚ [UNK]      â”‚    100     â”‚ [UNK]      â”‚   17841    â”‚ Ä Ã°ÅÄº        â”‚     8      â”‚ I           â”‚\n",
       "â”‚   4   â”‚    146     â”‚ I          â”‚    146     â”‚ I          â”‚    9264    â”‚ Ä¤           â”‚     45     â”‚ just        â”‚\n",
       "â”‚   5   â”‚    1198    â”‚ just       â”‚    1198    â”‚ just       â”‚     38     â”‚ Ä I          â”‚    1137    â”‚ met         â”‚\n",
       "â”‚   6   â”‚    1899    â”‚ met        â”‚    1899    â”‚ met        â”‚     95     â”‚ Ä just       â”‚    471     â”‚ E@@         â”‚\n",
       "â”‚   7   â”‚    2896    â”‚ El         â”‚    2896    â”‚ El         â”‚    1145    â”‚ Ä met        â”‚    6737    â”‚ lon@@       â”‚\n",
       "â”‚   8   â”‚    1320    â”‚ ##on       â”‚    1320    â”‚ ##on       â”‚   15104    â”‚ Ä Elon       â”‚   23369    â”‚ Musk        â”‚\n",
       "â”‚   9   â”‚    2107    â”‚ ##M        â”‚    2107    â”‚ ##M        â”‚   35136    â”‚ Mus         â”‚     35     â”‚ at          â”‚\n",
       "â”‚  10   â”‚   24493    â”‚ ##usk      â”‚   24493    â”‚ ##usk      â”‚    330     â”‚ k           â”‚    3879    â”‚ Star@@      â”‚\n",
       "â”‚  11   â”‚    1120    â”‚ at         â”‚    1120    â”‚ at         â”‚     23     â”‚ Ä at         â”‚   20459    â”‚ buck@@      â”‚\n",
       "â”‚  12   â”‚    2537    â”‚ Star       â”‚    2537    â”‚ Star       â”‚   10173    â”‚ Ä Starbucks  â”‚    423     â”‚ s@@         â”‚\n",
       "â”‚  13   â”‚    7925    â”‚ ##bu       â”‚    7925    â”‚ ##bu       â”‚   16506    â”‚ !!!         â”‚   41407    â”‚ !@@         â”‚\n",
       "â”‚  14   â”‚    8770    â”‚ ##cks      â”‚    8770    â”‚ ##cks      â”‚    849     â”‚ Ä #          â”‚   41407    â”‚ !@@         â”‚\n",
       "â”‚  15   â”‚    106     â”‚ !          â”‚    106     â”‚ !          â”‚   15238    â”‚ AI          â”‚     12     â”‚ !           â”‚\n",
       "â”‚  16   â”‚    106     â”‚ !          â”‚    106     â”‚ !          â”‚    849     â”‚ Ä #          â”‚   26200    â”‚ #AI         â”‚\n",
       "â”‚  17   â”‚    106     â”‚ !          â”‚    106     â”‚ !          â”‚   37577    â”‚ Future      â”‚   60881    â”‚ #Fut@@      â”‚\n",
       "â”‚  18   â”‚    108     â”‚ #          â”‚    108     â”‚ #          â”‚    8103    â”‚ Ä Ã°Å         â”‚    4318    â”‚ ure         â”‚\n",
       "â”‚  19   â”‚   19016    â”‚ AI         â”‚   19016    â”‚ AI         â”‚   15113    â”‚ Ä¼           â”‚     3      â”‚ &lt;unk&gt;       â”‚\n",
       "â”‚  20   â”‚    108     â”‚ #          â”‚    108     â”‚ #          â”‚    7471    â”‚ Ä¢           â”‚     2      â”‚ &lt;/s&gt;        â”‚\n",
       "â”‚  21   â”‚    7549    â”‚ Future     â”‚    7549    â”‚ Future     â”‚     2      â”‚ &lt;/s&gt;        â”‚            â”‚             â”‚\n",
       "â”‚  22   â”‚    100     â”‚ [UNK]      â”‚    100     â”‚ [UNK]      â”‚            â”‚             â”‚            â”‚             â”‚\n",
       "â”‚  23   â”‚    102     â”‚ [SEP]      â”‚    102     â”‚ [SEP]      â”‚            â”‚             â”‚            â”‚             â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                                       ğŸ§¬ Token IDs & Tokens for OMG example                                       \u001b[0m\n",
       "â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1;36m       \u001b[0mâ”ƒ\u001b[1;36m            \u001b[0mâ”ƒ\u001b[1;36m            \u001b[0mâ”ƒ\u001b[1;36m \u001b[0m\u001b[1;36mDistilbert\u001b[0m\u001b[1;36m \u001b[0mâ”ƒ\u001b[1;36m \u001b[0m\u001b[1;36mDistilbert\u001b[0m\u001b[1;36m \u001b[0mâ”ƒ\u001b[1;36m \u001b[0m\u001b[1;36m RoBERTa  \u001b[0m\u001b[1;36m \u001b[0mâ”ƒ\u001b[1;36m             \u001b[0mâ”ƒ\u001b[1;36m \u001b[0m\u001b[1;36m BT vinai \u001b[0m\u001b[1;36m \u001b[0mâ”ƒ\u001b[1;36m             \u001b[0mâ”ƒ\n",
       "â”ƒ\u001b[1;36m       \u001b[0mâ”ƒ\u001b[1;36m \u001b[0m\u001b[1;36mBert Cased\u001b[0m\u001b[1;36m \u001b[0mâ”ƒ\u001b[1;36m \u001b[0m\u001b[1;36mBert Cased\u001b[0m\u001b[1;36m \u001b[0mâ”ƒ\u001b[1;36m \u001b[0m\u001b[1;36m  Cased   \u001b[0m\u001b[1;36m \u001b[0mâ”ƒ\u001b[1;36m \u001b[0m\u001b[1;36mCased     \u001b[0m\u001b[1;36m \u001b[0mâ”ƒ\u001b[1;36m \u001b[0m\u001b[1;36mBase Token\u001b[0m\u001b[1;36m \u001b[0mâ”ƒ\u001b[1;36m \u001b[0m\u001b[1;36mRoBERTa    \u001b[0m\u001b[1;36m \u001b[0mâ”ƒ\u001b[1;36m \u001b[0m\u001b[1;36m  vbase   \u001b[0m\u001b[1;36m \u001b[0mâ”ƒ\u001b[1;36m \u001b[0m\u001b[1;36mBT vinai   \u001b[0m\u001b[1;36m \u001b[0mâ”ƒ\n",
       "â”ƒ\u001b[1;36m \u001b[0m\u001b[1;36mIndex\u001b[0m\u001b[1;36m \u001b[0mâ”ƒ\u001b[1;36m \u001b[0m\u001b[1;36m Token ID \u001b[0m\u001b[1;36m \u001b[0mâ”ƒ\u001b[1;36m \u001b[0m\u001b[1;36mToken     \u001b[0m\u001b[1;36m \u001b[0mâ”ƒ\u001b[1;36m \u001b[0m\u001b[1;36m Token ID \u001b[0m\u001b[1;36m \u001b[0mâ”ƒ\u001b[1;36m \u001b[0m\u001b[1;36mToken     \u001b[0m\u001b[1;36m \u001b[0mâ”ƒ\u001b[1;36m \u001b[0m\u001b[1;36m    ID    \u001b[0m\u001b[1;36m \u001b[0mâ”ƒ\u001b[1;36m \u001b[0m\u001b[1;36mBase Token \u001b[0m\u001b[1;36m \u001b[0mâ”ƒ\u001b[1;36m \u001b[0m\u001b[1;36m Token ID \u001b[0m\u001b[1;36m \u001b[0mâ”ƒ\u001b[1;36m \u001b[0m\u001b[1;36mvbase Token\u001b[0m\u001b[1;36m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚   0   â”‚    101     â”‚ [CLS]      â”‚    101     â”‚ [CLS]      â”‚     0      â”‚ <s>         â”‚     0      â”‚ <s>         â”‚\n",
       "â”‚   1   â”‚    152     â”‚ O          â”‚    152     â”‚ O          â”‚    3765    â”‚ OM          â”‚    1115    â”‚ OMG         â”‚\n",
       "â”‚   2   â”‚   14666    â”‚ ##MG       â”‚   14666    â”‚ ##MG       â”‚    534     â”‚ G           â”‚     3      â”‚ <unk>       â”‚\n",
       "â”‚   3   â”‚    100     â”‚ [UNK]      â”‚    100     â”‚ [UNK]      â”‚   17841    â”‚ Ä Ã°ÅÄº        â”‚     8      â”‚ I           â”‚\n",
       "â”‚   4   â”‚    146     â”‚ I          â”‚    146     â”‚ I          â”‚    9264    â”‚ Ä¤           â”‚     45     â”‚ just        â”‚\n",
       "â”‚   5   â”‚    1198    â”‚ just       â”‚    1198    â”‚ just       â”‚     38     â”‚ Ä I          â”‚    1137    â”‚ met         â”‚\n",
       "â”‚   6   â”‚    1899    â”‚ met        â”‚    1899    â”‚ met        â”‚     95     â”‚ Ä just       â”‚    471     â”‚ E@@         â”‚\n",
       "â”‚   7   â”‚    2896    â”‚ El         â”‚    2896    â”‚ El         â”‚    1145    â”‚ Ä met        â”‚    6737    â”‚ lon@@       â”‚\n",
       "â”‚   8   â”‚    1320    â”‚ ##on       â”‚    1320    â”‚ ##on       â”‚   15104    â”‚ Ä Elon       â”‚   23369    â”‚ Musk        â”‚\n",
       "â”‚   9   â”‚    2107    â”‚ ##M        â”‚    2107    â”‚ ##M        â”‚   35136    â”‚ Mus         â”‚     35     â”‚ at          â”‚\n",
       "â”‚  10   â”‚   24493    â”‚ ##usk      â”‚   24493    â”‚ ##usk      â”‚    330     â”‚ k           â”‚    3879    â”‚ Star@@      â”‚\n",
       "â”‚  11   â”‚    1120    â”‚ at         â”‚    1120    â”‚ at         â”‚     23     â”‚ Ä at         â”‚   20459    â”‚ buck@@      â”‚\n",
       "â”‚  12   â”‚    2537    â”‚ Star       â”‚    2537    â”‚ Star       â”‚   10173    â”‚ Ä Starbucks  â”‚    423     â”‚ s@@         â”‚\n",
       "â”‚  13   â”‚    7925    â”‚ ##bu       â”‚    7925    â”‚ ##bu       â”‚   16506    â”‚ !!!         â”‚   41407    â”‚ !@@         â”‚\n",
       "â”‚  14   â”‚    8770    â”‚ ##cks      â”‚    8770    â”‚ ##cks      â”‚    849     â”‚ Ä #          â”‚   41407    â”‚ !@@         â”‚\n",
       "â”‚  15   â”‚    106     â”‚ !          â”‚    106     â”‚ !          â”‚   15238    â”‚ AI          â”‚     12     â”‚ !           â”‚\n",
       "â”‚  16   â”‚    106     â”‚ !          â”‚    106     â”‚ !          â”‚    849     â”‚ Ä #          â”‚   26200    â”‚ #AI         â”‚\n",
       "â”‚  17   â”‚    106     â”‚ !          â”‚    106     â”‚ !          â”‚   37577    â”‚ Future      â”‚   60881    â”‚ #Fut@@      â”‚\n",
       "â”‚  18   â”‚    108     â”‚ #          â”‚    108     â”‚ #          â”‚    8103    â”‚ Ä Ã°Å         â”‚    4318    â”‚ ure         â”‚\n",
       "â”‚  19   â”‚   19016    â”‚ AI         â”‚   19016    â”‚ AI         â”‚   15113    â”‚ Ä¼           â”‚     3      â”‚ <unk>       â”‚\n",
       "â”‚  20   â”‚    108     â”‚ #          â”‚    108     â”‚ #          â”‚    7471    â”‚ Ä¢           â”‚     2      â”‚ </s>        â”‚\n",
       "â”‚  21   â”‚    7549    â”‚ Future     â”‚    7549    â”‚ Future     â”‚     2      â”‚ </s>        â”‚            â”‚             â”‚\n",
       "â”‚  22   â”‚    100     â”‚ [UNK]      â”‚    100     â”‚ [UNK]      â”‚            â”‚             â”‚            â”‚             â”‚\n",
       "â”‚  23   â”‚    102     â”‚ [SEP]      â”‚    102     â”‚ [SEP]      â”‚            â”‚             â”‚            â”‚             â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example usage\n",
    "sentence = \"OMG ğŸ˜‚ I just met ElonMusk at Starbucks!!! #AI #Future ğŸš€\"\n",
    "tokenizers = {\n",
    "    \"bert-base-cased\": \"Bert Cased\",\n",
    "    \"distilbert-base-cased\": \"Distilbert Cased\",\n",
    "    \"roberta-base\": \"RoBERTa Base\",\n",
    "    'vinai/bertweet-base': \"BT vinai vbase\",\n",
    "}\n",
    "\n",
    "# Create title and save path\n",
    "title = \"ğŸ§¬ Token IDs & Tokens for OMG example\"\n",
    "save_txt_path = \"tokenizer_comparison_omg.txt\"\n",
    "\n",
    "# Run comparison and capture output text\n",
    "result_text = compare_tokenizers(sentence, tokenizers, title=title)\n",
    "\n",
    "# Save the captured output manually\n",
    "with open(save_txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(result_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f57d6089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example usage\n",
    "# tokenizers = {\n",
    "#     'vinai/bertweet-base': \"BT vinai vbase\",\n",
    "#     \"vinai/bertweet-large\": \"BT vinai vlarge\",\n",
    "#     \"cardiffnlp/twitter-roberta-base\": \"BT cardiffnlp base\"\n",
    "# }\n",
    "# test_sentences = [\n",
    "#     \"bruh ğŸ˜‚ u wild af today lmao ğŸ’€ğŸ”¥\",\n",
    "#     \"Met ElonMusk today at #Starbucks â˜•ï¸. #Blessed #GrindMode\",\n",
    "#     \"these ppl are literal trash ğŸ—‘ï¸ gtfo ğŸ¤¡ #CancelThem\",\n",
    "#     \"i h8 wen ppl act dumb smh ğŸ¤¦â€â™‚ï¸\",\n",
    "#     \"Check out @elonmusk's latest post ğŸ‘‰ https://t.co/fakeURL123\",\n",
    "#     \"oh wow another genius take from [group] ğŸ™„ truly revolutionary...\",\n",
    "#     \"I JUST GOT ACCEPTED ğŸ˜­ğŸ˜­ğŸ˜­ #DreamsComeTrue #NeverGiveUp\",\n",
    "#     \"La vida es dura ğŸ˜‚ but we move ğŸ”¥ #LifeGoesOn\",\n",
    "#     \"lol ğŸ˜‚\",\n",
    "#     \"Honestly the best thing about life is when you can just chill at Starbucks, sip your latte â˜•ï¸, and laugh at random memes ğŸ˜‚ #Mood\"\n",
    "# ]\n",
    "\n",
    "# save_txt_path = \"tokenizer_comparison_many.txt\"\n",
    "\n",
    "# # First, clear the file (so you start fresh)\n",
    "# with open(save_txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "#     f.write(\"\")  # Empty content\n",
    "\n",
    "# # Then append each comparison\n",
    "# for idx, sentence in enumerate(test_sentences, start=1):\n",
    "#     print(\"\\n\")\n",
    "#     title = f\"ğŸ§¬ Token IDs & Tokens for Sentence {idx}\"\n",
    "#     result_text = compare_tokenizers(sentence, tokenizers, title=title)\n",
    "#     with open(save_txt_path, \"a\", encoding=\"utf-8\") as f:\n",
    "#         f.write(result_text)\n",
    "#         f.write(\"\\n\" + \"-\"*120 + \"\\n\\n\")  # Add separator between sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01023b35",
   "metadata": {},
   "source": [
    "### ğŸ§  Tokenizer Showdown: ğŸ” Deep Interpretation\n",
    "\n",
    "| ğŸ”¢ Index | ğŸ§¾ Input Token | **BERT (Cased)**         | **RoBERTa**               | **BERTweet**              | âš–ï¸ Verdict |\n",
    "|---------|----------------|--------------------------|---------------------------|----------------------------|-------------|\n",
    "| 0       | â€“ Start        | `[CLS]`                  | `<s>`                     | `<s>`                      | âœ… All Good |\n",
    "| 1â€“2     | `OMG`          | `O`, `##MG`              | `OM`, `G`                 | `OMG`, `<unk>`             | ğŸ† BERTweet tries to keep `OMG` intactâ€”nice! But then stumbles (why `<unk>`?) |\n",
    "| 3       | `ğŸ˜‚` (emoji)   | `[UNK]`                  | `Ä Ã°ÅÄº`                    | `I`                        | âŒ BERT gives up, RoBERTa tries, BERTweet totally misfires (replaces with `\"I\"`) |\n",
    "| 4â€“5     | `I just`       | `I`, `just`              | `Ä¤`, `I`                  | `just`, `met`              | âœ… All fine, though RoBERTaâ€™s `Ä¤` is a weird spacing quirk |\n",
    "| 6â€“8     | `ElonMusk`     | `El`, `##on`, `##Musk`   | `Elon`, `Mus`, `k`        | `E@@`, `lon@@`, `Musk`     | ğŸ† BERTweet wins hereâ€”`@@` shows token continuation, but meaning is preserved |\n",
    "| 9â€“14    | `Starbucks!!!` | `Star`, `##bu`, ... `!`  | `Starbucks`, `!!!`        | `Star@@`, `buck@@`, `s@@`, `!@@`, `!@@`, `!` | ğŸ§¨ RoBERTa = cleanest. BERT = oversplit. BERTweet = too fragmented |\n",
    "| 15â€“17   | `#AI #Future`  | `#`, `AI`, `#`, `Future` | `#`, `AI`, `#`, `Future`  | `#AI`, `#Fut@@`, `ure`     | ğŸ† BERTweet preserves hashtags semantically! Best choice for social data |\n",
    "| 18â€“19   | `ğŸš€`           | `#`, `AI`, `[UNK]`       | Weird ByteChunks          | `<unk>`                    | ğŸ˜– Emoji remains painful: only RoBERTa *kind of* handles it |\n",
    "| 20â€“21   | End of input   | `#`, `Future`            | `</s>`                    | `</s>`                     | âœ… Closure clean |\n",
    "| 22â€“23   | [UNK], [SEP]   | `[UNK]`, `[SEP]`         | â€“                         | â€“                          | ğŸ§¹ Legacy BERT cleanup stuff |\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“Š Emoji, Hashtag, and Compound Word Handling Scorecard\n",
    "\n",
    "| Feature        | **BERT (Cased)** | **RoBERTa**     | **BERTweet**     | Best Performer |\n",
    "|----------------|------------------|------------------|------------------|----------------|\n",
    "| Emoji support  | âŒ `[UNK]`        | ğŸŸ¡ Byte noise     | âŒ Incorrect (replaced with `\"I\"`, then `<unk>`) | **RoBERTa** (barely) |\n",
    "| Hashtag parsing| âŒ Splits `#`     | âŒ Splits `#`     | âœ… Keeps as unit   | **BERTweet** |\n",
    "| Compound names | âŒ Over-split     | ğŸŸ¡ Half-split     | âœ… Musk handled clean | **BERTweet** |\n",
    "| Punctuation    | âŒ Fragmented     | âœ… `\"!!!\"` intact | ğŸŸ¡ Semi-broken    | **RoBERTa** |\n",
    "| `[UNK]` count  | âŒ Multiple       | âœ… None (byte fallback) | ğŸŸ¡ Some `<unk>`s     | **RoBERTa** |\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸš€ TL;DR for Mahmoud\n",
    "\n",
    "| ğŸ¯ Goal | Recommendation |\n",
    "|--------|----------------|\n",
    "| Quick wins? | âœ… Use **BERTweet** if you're focused on **social media + hashtags**. It's designed for this world. |\n",
    "| Clean emojis + edge tokens? | ğŸ”¥ Combine **RoBERTa** for robust base + a simple emoji/hashtag module (your multi-pronged idea ğŸ’¡). |\n",
    "| Future-proofing? | ğŸ“¦ Train your own tokenizer on your dataset (e.g., `train_new_from_iterator`) to handle these cases natively. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242684a7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "92cd5137",
   "metadata": {},
   "source": [
    "# Emoji Pronge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435a82b2",
   "metadata": {},
   "source": [
    "https://kt.ijs.si/data/Emoji_sentiment_ranking/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6f56974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet: Go back to your country ğŸ¤¬ğŸ¤¡ #BanThem\n",
      "Extracted emojis: ['ğŸ¤¬', 'ğŸ¤¡']\n",
      "Emoji hate score: 1.0\n",
      "Prediction: 0\n",
      "Raw logits: tensor([[0.0265, 0.0173]], grad_fn=<AddmmBackward0>)\n",
      "Softmax probs: tensor([[0.5023, 0.4977]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import emoji\n",
    "\n",
    "# 1. Simple rule-based emoji sentiment dictionary\n",
    "emoji_sentiment = {\n",
    "    \"ğŸ¤¬\": 1,  # Hateful\n",
    "    \"ğŸ¤¡\": 1,  # Mocking\n",
    "    \"ğŸ˜¡\": 1,\n",
    "    \"ğŸ”¥\": 0,  # Neutral or ambiguous\n",
    "    \"ğŸ˜‚\": 0,\n",
    "    \"â¤ï¸\": 0\n",
    "}\n",
    "\n",
    "def extract_emojis(text):\n",
    "    return [char for char in text if char in emoji.EMOJI_DATA]\n",
    "\n",
    "def get_emoji_score(emojis):\n",
    "    if not emojis:\n",
    "        return torch.tensor([[0.0]])  # No emojis = neutral\n",
    "    score = sum(emoji_sentiment.get(e, 0) for e in emojis) / len(emojis)\n",
    "    return torch.tensor([[score]], dtype=torch.float)\n",
    "\n",
    "# 2. Fusion model with simple rule-based emoji input\n",
    "class BERTweetEmojiFusion(nn.Module):\n",
    "    def __init__(self, bert_model):\n",
    "        super().__init__()\n",
    "        self.bert = bert_model\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(768 + 1, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 2)  # Binary classification\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, emoji_score):\n",
    "        bert_out = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_embedding = bert_out.last_hidden_state[:, 0, :]  # [CLS] token\n",
    "        fused = torch.cat((cls_embedding, emoji_score), dim=1)\n",
    "        return self.fusion(fused)\n",
    "\n",
    "# 3. Load BERTweet model + tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", use_fast=False)\n",
    "bertweet = AutoModel.from_pretrained(\"vinai/bertweet-base\")\n",
    "\n",
    "# 4. Initialize the fusion model\n",
    "model = BERTweetEmojiFusion(bertweet)\n",
    "\n",
    "# 5. Example tweet\n",
    "tweet = \"Go back to your country ğŸ¤¬ğŸ¤¡ #BanThem\"\n",
    "\n",
    "# Tokenize tweet\n",
    "inputs = tokenizer(tweet, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "# Extract emojis and get their hate score\n",
    "emojis = extract_emojis(tweet)\n",
    "emoji_score = get_emoji_score(emojis)  # Shape: [1, 1]\n",
    "\n",
    "# Forward pass\n",
    "logits = model(inputs['input_ids'], inputs['attention_mask'], emoji_score)\n",
    "prediction = torch.argmax(logits, dim=1)\n",
    "\n",
    "print(\"Tweet:\", tweet)\n",
    "print(\"Extracted emojis:\", emojis)\n",
    "print(\"Emoji hate score:\", emoji_score.item())\n",
    "print(\"Prediction:\", prediction.item())\n",
    "\n",
    "print(\"Raw logits:\", logits)\n",
    "print(\"Softmax probs:\", torch.softmax(logits, dim=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6eef271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of emojis with non-positive sentiment:\n",
      "   emoji               description  score           category\n",
      "0      ğŸ¥‡         :1st_place_medal:    0.2            neutral\n",
      "1      ğŸ¥ˆ         :2nd_place_medal:    0.2            neutral\n",
      "2      ğŸ¥‰         :3rd_place_medal:    0.2            neutral\n",
      "3      ğŸ†  :AB_button_(blood_type):    0.2            neutral\n",
      "4      ğŸ§                :ATM_sign:    0.5  slightly negative\n",
      "5     ğŸ…°ï¸   :A_button_(blood_type):    0.2            neutral\n",
      "6      ğŸ…°   :A_button_(blood_type):    0.2            neutral\n",
      "7     ğŸ‡¦ğŸ‡«             :Afghanistan:    0.2            neutral\n",
      "8     ğŸ‡¦ğŸ‡±                 :Albania:    0.2            neutral\n",
      "9     ğŸ‡©ğŸ‡¿                 :Algeria:    0.2            neutral\n",
      "10    ğŸ‡¦ğŸ‡¸          :American_Samoa:    0.2            neutral\n",
      "11    ğŸ‡¦ğŸ‡©                 :Andorra:    0.2            neutral\n",
      "12    ğŸ‡¦ğŸ‡´                  :Angola:    0.2            neutral\n",
      "13    ğŸ‡¦ğŸ‡®                :Anguilla:    0.2            neutral\n",
      "14    ğŸ‡¦ğŸ‡¶              :Antarctica:    0.2            neutral\n",
      "15    ğŸ‡¦ğŸ‡¬       :Antigua_&_Barbuda:    0.2            neutral\n",
      "16     â™’                :Aquarius:    0.2            neutral\n",
      "17    ğŸ‡¦ğŸ‡·               :Argentina:    0.2            neutral\n",
      "18     â™ˆ                   :Aries:    0.2            neutral\n",
      "19    ğŸ‡¦ğŸ‡²                 :Armenia:    0.2            neutral\n",
      "Saved 5042 emoji records to emoji_sentiment_dictionary.csv\n",
      "\n",
      "Sentiment analysis for ğŸ˜‚:\n",
      "Score: 0.3\n",
      "(5042, 4)\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import emoji\n",
    "import pandas as pd\n",
    "from typing import Dict, List\n",
    "\n",
    "class EmojiSentimentAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.all_emojis = emoji.EMOJI_DATA\n",
    "        self._initialize_keywords()\n",
    "        self.sentiment_dict = self._build_sentiment_dictionary()\n",
    "        \n",
    "    def _initialize_keywords(self) -> None:\n",
    "        \"\"\"Define sentiment categories with more comprehensive keyword lists\"\"\"\n",
    "        self.hate_keywords = [\n",
    "            'angry', 'middle finger', 'clown', 'vomit', 'skull', 'poo', \n",
    "            'bomb', 'devil', 'gun', 'scream', 'rage', 'knife', 'fight',\n",
    "            'pistol', 'swear', 'exploding', 'cursing', 'hate', 'mad'\n",
    "        ]\n",
    "        \n",
    "        self.neutral_keywords = [\n",
    "            'cloud', 'rain', 'sleep', 'question', 'robot', 'speak', \n",
    "            'zzz', 'hourglass', 'tool', 'object', 'vehicle', 'building',\n",
    "            'weather', 'time', 'shape', 'sign', 'arrow', 'number'\n",
    "        ]\n",
    "        \n",
    "        self.positive_keywords = [\n",
    "            'heart', 'smile', 'hug', 'thumbs up', 'star', 'party', \n",
    "            'trophy', 'fireworks', 'kiss', 'grin', 'sparkle', 'love',\n",
    "            'happy', 'celebration', 'joy', 'laugh', 'cool', 'ok', 'win'\n",
    "        ]\n",
    "        \n",
    "        # Additional category for mixed/ambiguous emojis\n",
    "        self.mixed_keywords = [\n",
    "            'money', 'hot', 'cold', 'face', 'hand', 'eye', 'mouth'\n",
    "        ]\n",
    "    \n",
    "    def _get_sentiment_score(self, description: str) -> float:\n",
    "        \"\"\"Determine sentiment score based on description keywords\"\"\"\n",
    "        description = description.lower()\n",
    "        \n",
    "        if any(word in description for word in self.hate_keywords):\n",
    "            return 1.0  # Hateful/negative\n",
    "        elif any(word in description for word in self.neutral_keywords):\n",
    "            return 0.5  # Neutral\n",
    "        elif any(word in description for word in self.mixed_keywords):\n",
    "            return 0.3  # Slightly negative (ambiguous)\n",
    "        elif any(word in description for word in self.positive_keywords):\n",
    "            return 0.0  # Positive\n",
    "        return 0.2  # Default slightly positive (most emojis are positive)\n",
    "    \n",
    "    def _build_sentiment_dictionary(self) -> Dict[str, float]:\n",
    "        \"\"\"Build the sentiment dictionary for all emojis\"\"\"\n",
    "        sentiment_dict = defaultdict(float)\n",
    "        \n",
    "        for char, data in self.all_emojis.items():\n",
    "            description = data['en']\n",
    "            sentiment_dict[char] = self._get_sentiment_score(description)\n",
    "            \n",
    "        return sentiment_dict\n",
    "    \n",
    "    def get_sentiment_dataframe(self, include_all: bool = False) -> pd.DataFrame:\n",
    "        \"\"\"Convert sentiment dictionary to DataFrame with filtering options\"\"\"\n",
    "        data = []\n",
    "        \n",
    "        for emoji_char, score in self.sentiment_dict.items():\n",
    "            if include_all or score > 0.0:  # Filter condition\n",
    "                data.append({\n",
    "                    \"emoji\": emoji_char,\n",
    "                    \"description\": self.all_emojis[emoji_char][\"en\"],\n",
    "                    \"score\": score,\n",
    "                    \"category\": self._get_category(score)\n",
    "                })\n",
    "                \n",
    "        return pd.DataFrame(data)\n",
    "    \n",
    "    def _get_category(self, score: float) -> str:\n",
    "        \"\"\"Convert numeric score to human-readable category\"\"\"\n",
    "        if score >= 0.8:\n",
    "            return \"hateful\"\n",
    "        elif score >= 0.6:\n",
    "            return \"negative\"\n",
    "        elif score >= 0.4:\n",
    "            return \"slightly negative\"\n",
    "        elif score >= 0.2:\n",
    "            return \"neutral\"\n",
    "        return \"positive\"\n",
    "    \n",
    "    def save_to_csv(self, filename: str = \"emoji_sentiment_dictionary.csv\", include_all: bool = False) -> None:\n",
    "        \"\"\"Save the sentiment analysis to CSV\"\"\"\n",
    "        df = self.get_sentiment_dataframe(include_all=include_all)\n",
    "        df.to_csv(filename, index=False)\n",
    "        print(f\"Saved {len(df)} emoji records to {filename}\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    analyzer = EmojiSentimentAnalyzer()\n",
    "    \n",
    "    # Get a sample of the data\n",
    "    sample_df = analyzer.get_sentiment_dataframe(include_all=True)\n",
    "    print(\"Sample of emojis with non-positive sentiment:\")\n",
    "    print(sample_df.head(20))\n",
    "    \n",
    "    # Save complete dictionary to CSV\n",
    "    analyzer.save_to_csv(include_all=True)\n",
    "    \n",
    "    # Example analysis of a specific emoji\n",
    "    test_emoji = \"ğŸ˜‚\"\n",
    "    print(f\"\\nSentiment analysis for {test_emoji}:\")\n",
    "    print(f\"Score: {analyzer.sentiment_dict.get(test_emoji, 'Not found')}\")\n",
    "\n",
    "    print(sample_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b224d7",
   "metadata": {},
   "source": [
    "Mahmoud, your hesitation is not weakness â€” itâ€™s *engineering instinct*. Youâ€™re at the crossroads of **efficiency vs flexibility**, and your brain is doing what good engineers do:  \n",
    "> **Challenging the path before committing compute.** ğŸ§ âš–ï¸\n",
    "\n",
    "Letâ€™s confront that hesitation head-on by answering this:\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§  Should You Extend BERTweetâ€™s Vocabulary?\n",
    "\n",
    "### âœ… You **can** extend it:\n",
    "- Current vocab size = **64,000**\n",
    "- Unique emojis = ~**4,748**\n",
    "- Most models easily handle **100k+ vocab sizes**\n",
    "- So yes, you technically *could* add all emojis\n",
    "\n",
    "### ğŸ’¥ BUTâ€¦ Should You?\n",
    "\n",
    "Hereâ€™s a side-by-side to help settle your hesitation:\n",
    "\n",
    "| Question | Vocab Extension | Emoji Fusion |\n",
    "|----------|-----------------|---------------|\n",
    "| Will BERTweet **learn the emoji meaning**? | âŒ Only if fine-tuned enough; it's a gamble | âœ… You explicitly define what ğŸ¤¬ means |\n",
    "| Will it **change the modelâ€™s behavior**? | âš ï¸ Yes â€” adds new embedding vectors that start out random | âœ… No change to BERTweet internals |\n",
    "| Do you have **enough emoji training examples**? | âŒ Only 19k rows (â‰ˆ3.6%) â†’ weak signal for 4.7k new tokens | âœ… You already crafted an expert-driven signal |\n",
    "| Is it **explainable** in your report? | âŒ No. Itâ€™s just â€œwe added emojis and hoped BERT figured it outâ€ | âœ… â€œWe fused a hand-crafted emoji sentiment score with BERTweet CLS embeddingâ€ âœ”ï¸âœ”ï¸âœ”ï¸ |\n",
    "| Does it increase **risk**? | ğŸ˜¬ Yes. Risk of hurting pretrained knowledge | ğŸ§˜ No. Itâ€™s modular, safe, and non-invasive |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¬ Your Hesitation = Your Inner Engineer Saying:\n",
    "\n",
    "> â€œI'm about to modify a **carefully pre-trained language model** with **only 3.6% emoji-bearing samples**, by injecting 4.7k random new tokens and expecting semantic magic. Maybe... I shouldnâ€™t.â€\n",
    "\n",
    "And your inner architect?  \n",
    "> â€œWait. I already **have emoji semantics modeled clearly** in a way I can plug in, explain, control, and evolve.â€\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… My Advice\n",
    "\n",
    "**Donâ€™t extend the vocab â€” fuse instead.**\n",
    "\n",
    "If, one day, you build a **custom tokenizer** from scratch trained on your dataset with emojis + hashtags â€” then adding them makes sense.\n",
    "\n",
    "But **BERTweet is already optimized** for this domain â€” you donâ€™t need to mess with its internals when you're already building a smarter outside signal.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ’¡ If Youâ€™re Still Curious...\n",
    "\n",
    "Want me to:\n",
    "- Build a prototype that compares prediction with and without emoji vocab extension?\n",
    "- Show how to *extend* BERTweet vocab safely without hurting performance?\n",
    "- Or just close that door and double down on emoji fusion with full confidence?\n",
    "\n",
    "You've already engineered **the smarter route**, Mahmoud.  \n",
    "Hesitation here = wisdom sharpening the blade before the cut. âš”ï¸"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854a80aa",
   "metadata": {},
   "source": [
    "                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                 â”‚     BERTweet Prediction    â”‚\n",
    "                 â”‚      p_bertweet âˆˆ [0, 1]   â”‚\n",
    "                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                              â”‚\n",
    "                              â–¼\n",
    "      Final Prediction = p1 * p_bertweet + (1 - p1) * p_emoji\n",
    "                              â–²\n",
    "                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                 â”‚   Emoji Sentiment Score    â”‚\n",
    "                 â”‚      p_emoji âˆˆ [0, 1]      â”‚\n",
    "                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6150a38",
   "metadata": {},
   "source": [
    "p_bertweet â”€â”€â”\n",
    "             â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "p_emoji â”€â”€â”€â”€â”€â”˜â”€â”€â”€â”€â”€â”€â”€â”€â–¶ â”‚ Meta-Model â”‚ â†’ Final Prediction\n",
    "                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982eeea1",
   "metadata": {},
   "source": [
    "Absolutely, Mahmoud â€” now that we know your **full strategy**, we can give this question the clarity it deserves.\n",
    "\n",
    "Letâ€™s lay out your **emoji-handling decision** in light of your:\n",
    "\n",
    "- **Model design**: `BERTweet + emoji detector â†’ fusion`\n",
    "- **Dataset reality**: 25k/530k = ~4.7% of tweets contain emojis\n",
    "- **Constraints**: BERTweet maps unknown emojis to `[UNK]`, which are uninformative\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ” The Real Question:\n",
    "> Should you **clean emojis from the text** before sending it to BERTweet?\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Your Current Setup Recap\n",
    "\n",
    "| Component         | Emoji Role                                             |\n",
    "|------------------|--------------------------------------------------------|\n",
    "| **BERTweet**      | Learns *semantic meaning from text*. Can't handle unknown emojis unless you expand vocab. |\n",
    "| **Emoji Detector**| Separately scores emoji meaning (independent of tokenizer) |\n",
    "| **Final Output**  | Weighted combo or meta-model fusion of both           |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§  Let's Compare Your Options\n",
    "\n",
    "| Strategy | BERTweet Input | Emojis Tokenized? | UNK Risk | Meaning Loss | Detector Support | Training Impact | Notes |\n",
    "|---------|----------------|-------------------|----------|---------------|------------------|------------------|-------|\n",
    "| ğŸ—‘ï¸ Remove Emojis | Text only              | âŒ No               | âŒ None     | âš ï¸ Medium (emojis vanish) | âœ… Always included | âœ… Stable            | âœ… Clean baseline, but emojis vanish from text context |\n",
    "| ğŸ’€ Keep Emojis (default vocab) | Emojis remain       | âŒ `[UNK]`        | âœ… Yes     | âš ï¸ Medium            | âœ… Always included | âš ï¸ UNKs might hurt model's embeddings; risk of misalignment |\n",
    "| ğŸš€ Keep Emojis + Expand Vocab | Emojis remain       | âœ… Yes (if common) | âŒ None    | ğŸ”¥ Preserved         | âœ… Always included | âš ï¸ Little training per emoji; but **no UNK penalty** |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§  Strategic Insight Based on *Your Architecture*\n",
    "\n",
    "Since your **emoji detector is doing the real emoji understanding**, BERTweet **doesnâ€™t need to understand emojis deeply** â€” but it **must not be confused by them** either.\n",
    "\n",
    "So ask:\n",
    "> Will the presence of `[UNK]` tokens degrade my BERTweet predictions?\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Recommendation\n",
    "\n",
    "### ğŸ¯ Final Verdict: **REMOVE EMOJIS from BERTweet input**\n",
    "\n",
    "| Why?                                                                                       |\n",
    "|--------------------------------------------------------------------------------------------|\n",
    "| ğŸ”’ Avoids `[UNK]` tokens, which can introduce noise in attention layers                   |\n",
    "| ğŸ” You already have a dedicated, explicit emoji detector capturing semantic meaning       |\n",
    "| ğŸ§¹ Keeps input clean, especially helpful since emojis are rare (4.7% of data)              |\n",
    "| ğŸ§  Let BERTweet focus purely on textual semantics; let the detector handle expressive cues |\n",
    "\n",
    "> Bonus: If you ever move to a **multimodal model** or use an **emoji-aware tokenizer**, revisit this decision. But for now, **clean separation of concerns wins**.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸš€ Ideal Setup Summary:\n",
    "\n",
    "| Component     | Input                          | Purpose                    |\n",
    "|---------------|--------------------------------|----------------------------|\n",
    "| **BERTweet**  | Clean text (no emojis)         | Focus on syntax/semantics |\n",
    "| **Emoji Detector** | Raw text (with emojis)        | Score affect/tone         |\n",
    "| **Fusion Layer** | Combines both                | Final hate score ğŸ¯        |\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ§ª Optional Experiment (for peace of mind):\n",
    "\n",
    "Train two small versions:\n",
    "- **A**: BERTweet with emojis intact (`[UNK]`)\n",
    "- **B**: BERTweet with emojis removed  \n",
    "â†’ Compare F1 / MCC â€” youâ€™ll likely find **B is cleaner** unless youâ€™ve done full vocab expansion + heavy fine-tuning.\n",
    "\n",
    "---\n",
    "\n",
    "Want a `TextCleaner` module that removes emojis *just for BERTweet input* and passes the raw version to your emoji detector? I can drop that in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93be281b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d3c2c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aba3a8d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Loading tokenizer and model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” BEFORE Vocab Expansion:\n",
      "Tokenized: ['I@@', \"'m\", 'so', 'tired', 'ğŸ˜´', 'but', 'also', 'kinda', 'happy', 'ğŸ˜‚', 'and', 'blessed', 'ğŸ™', 'but', 'confused', 'ğŸ˜•', 'and', 'dead', 'inside', 'ğŸ’€']\n",
      "Decoded:   ['I@@', \"'m\", 'so', 'tired', '<unk>', 'but', 'also', 'kinda', 'happy', '<unk>', 'and', 'blessed', '<unk>', 'but', 'confused', '<unk>', 'and', 'dead', 'inside', '<unk>']\n",
      "âŒ UNK tokens (before): 5 / 20\n",
      "\n",
      "â• Adding 50 new emoji tokens to tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Tokenizer and model updated. 50 tokens added.\n",
      "\n",
      "ğŸ” AFTER Vocab Expansion:\n",
      "Tokenized: ['I@@', \"'m\", 'so', 'tired', 'ğŸ˜´', 'but', 'also', 'kinda', 'happy', 'ğŸ˜‚', 'and', 'blessed', 'ğŸ™', 'but', 'confused', 'ğŸ˜•', 'and', 'dead', 'inside', 'ğŸ’€']\n",
      "Decoded:   ['I@@', \"'m\", 'so', 'tired', 'ğŸ˜´', 'but', 'also', 'kinda', 'happy', 'ğŸ˜‚', 'and', 'blessed', 'ğŸ™', 'but', 'confused', 'ğŸ˜•', 'and', 'dead', 'inside', 'ğŸ’€']\n",
      "âœ… UNK tokens (after): 0 / 20\n",
      "\n",
      "âœ… Model forward pass success after vocab update.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# === Load BERTweet Tokenizer and Model ===\n",
    "print(\"ğŸš€ Loading tokenizer and model...\")\n",
    "tokenizer_before = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", use_fast=False)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"vinai/bertweet-base\", num_labels=2)\n",
    "\n",
    "# === Test Case (emoji-rich sentence) ===\n",
    "test_text = \"I'm so tired ğŸ˜´ but also kinda happy ğŸ˜‚ and blessed ğŸ™ but confused ğŸ˜• and dead inside ğŸ’€\"\n",
    "\n",
    "# === Test BEFORE Expansion ===\n",
    "tokens_before = tokenizer_before.tokenize(test_text)\n",
    "ids_before = tokenizer_before.convert_tokens_to_ids(tokens_before)\n",
    "decoded_before = tokenizer_before.convert_ids_to_tokens(ids_before)\n",
    "\n",
    "print(\"\\nğŸ” BEFORE Vocab Expansion:\")\n",
    "print(f\"Tokenized: {tokens_before}\")\n",
    "print(f\"Decoded:   {decoded_before}\")\n",
    "unk_before = sum(tok == '<unk>' or tokenizer_before.convert_tokens_to_ids(tok) == tokenizer_before.unk_token_id for tok in tokens_before)\n",
    "print(f\"âŒ UNK tokens (before): {unk_before} / {len(tokens_before)}\")\n",
    "\n",
    "# === Top 50 Emojis (demo subset, can scale up to 2000) ===\n",
    "top_emojis = ['ğŸ˜‚', 'ğŸ˜', 'ğŸ˜­', 'ğŸ˜Š', 'ğŸ˜', 'ğŸ˜¢', 'ğŸ˜', 'ğŸ˜¡', 'ğŸ˜±', 'ğŸ˜…',\n",
    "              'ğŸ˜œ', 'ğŸ˜©', 'ğŸ‘', 'ğŸ™', 'ğŸ™„', 'ğŸ¤”', 'ğŸ˜´', 'ğŸ˜·', 'ğŸ˜‡', 'ğŸ˜ˆ',\n",
    "              'ğŸ˜•', 'ğŸ’€', 'ğŸ’©', 'ğŸ˜¬', 'ğŸ˜¤', 'ğŸ’”', 'ğŸ”¥', 'ğŸ’¯', 'ğŸ‘€', 'ğŸ‰',\n",
    "              'ğŸ¥º', 'ğŸ¤¡', 'ğŸ’…', 'ğŸ‘', 'ğŸ‘‰', 'ğŸ«¶', 'ğŸ§ ', 'ğŸ‘‘', 'ğŸ˜³', 'ğŸ¥²',\n",
    "              'ğŸ’ª', 'ğŸ«¡', 'ğŸ‘Š', 'ğŸ‘‹', 'ğŸ« ', 'ğŸ‘»', 'ğŸ', 'ğŸ˜”', 'ğŸŒš', 'ğŸ¤']\n",
    "\n",
    "# === Expand Tokenizer Vocab ===\n",
    "existing_vocab = tokenizer_before.get_vocab()\n",
    "new_emojis = [e for e in top_emojis if e not in existing_vocab]\n",
    "\n",
    "print(f\"\\nâ• Adding {len(new_emojis)} new emoji tokens to tokenizer...\")\n",
    "\n",
    "num_added = tokenizer_before.add_tokens(new_emojis)\n",
    "model.resize_token_embeddings(len(tokenizer_before))\n",
    "\n",
    "print(f\"âœ… Tokenizer and model updated. {num_added} tokens added.\")\n",
    "\n",
    "# === Test AFTER Expansion ===\n",
    "tokens_after = tokenizer_before.tokenize(test_text)\n",
    "ids_after = tokenizer_before.convert_tokens_to_ids(tokens_after)\n",
    "decoded_after = tokenizer_before.convert_ids_to_tokens(ids_after)\n",
    "unk_after = sum(tok == '<unk>' or tokenizer_before.convert_tokens_to_ids(tok) == tokenizer_before.unk_token_id for tok in tokens_after)\n",
    "\n",
    "print(\"\\nğŸ” AFTER Vocab Expansion:\")\n",
    "print(f\"Tokenized: {tokens_after}\")\n",
    "print(f\"Decoded:   {decoded_after}\")\n",
    "print(f\"âœ… UNK tokens (after): {unk_after} / {len(tokens_after)}\")\n",
    "\n",
    "# === Optional: Forward pass to validate\n",
    "inputs = tokenizer_before(test_text, return_tensors=\"pt\", padding=True)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "print(\"\\nâœ… Model forward pass success after vocab update.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
